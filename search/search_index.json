{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"xgbse : XGBoost Survival Embeddings \u00b6 \"There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.\" - Leo Breiman, Statistical Modeling: The Two Cultures Survival Analysis is a powerful statistical technique with a wide range of applications such as predictive maintenance, customer churn, credit risk, asset liquidity risk, and others. However, it has not yet seen widespread adoption in industry, with most implementations embracing one of two cultures: models with sound statistical properties, but lacking in expressivess and computational efficiency highly efficient and expressive models, but lacking in statistical rigor xgbse aims to unite the two cultures in a single package, adding a layer of statistical rigor to the highly expressive and computationally effcient xgboost survival analysis implementation. The package offers: calibrated and unbiased survival curves with confidence intervals (instead of point predictions) great predictive power, competitive to vanilla xgboost efficient, easy to use implementation explainability through prototypes This is a research project by Loft Data Science Team , however we invite the community to contribute. Please help by trying it out, reporting bugs, and letting us know what you think! [TODO] Installation \u00b6 ... Usage \u00b6 Basic usage \u00b6 The package follows scikit-learn API, with a minor adaptation to work with time and event data ( y as a numpy structured array of times and events). .predict() returns a dataframe where each column is a time window and values represent the probability of survival before or exactly at the time window. # importing dataset from pycox package from pycox.datasets import metabric # importing model and utils from xgbse from xgbse import XGBEmbedKaplanNeighbors from xgbse.converters import convert_to_structured # getting data df = metabric . read_df () # splitting to X, y format X = df . drop ([ 'duration' , 'event' ], axis = 1 ) y = convert_to_structured ( df [ 'duration' ], df [ 'event' ]) # fitting xgbse model xgbse_model = XGBEmbedKaplanNeighbors ( n_neighbors = 50 ) xgbse_model . fit ( X , y ) # predicting event_probs = xgbse_model . predict ( X ) event_probs . head () index 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 0 0.98 0.87 0.81 0.74 0.71 0.66 0.53 0.47 0.42 0.4 0.3 0.25 0.21 0.16 0.12 0.098 0.085 0.062 0.054 1 0.99 0.89 0.79 0.7 0.64 0.58 0.53 0.46 0.42 0.39 0.33 0.31 0.3 0.24 0.21 0.18 0.16 0.11 0.095 2 0.94 0.78 0.63 0.57 0.54 0.49 0.44 0.37 0.34 0.32 0.26 0.23 0.21 0.16 0.13 0.11 0.098 0.072 0.062 3 0.99 0.95 0.93 0.88 0.84 0.81 0.73 0.67 0.57 0.52 0.45 0.37 0.33 0.28 0.23 0.19 0.16 0.12 0.1 4 0.98 0.92 0.82 0.77 0.72 0.68 0.63 0.6 0.57 0.55 0.51 0.48 0.45 0.42 0.38 0.33 0.3 0.22 0.2 You can also get interval predictions (probability of failing exactly at each time window) using return_interval_probs : # point predictions interval_probs = xgbse_model . predict ( X_valid , return_interval_probs = True ) interval_probs . head () index 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 0 0.024 0.1 0.058 0.07 0.034 0.05 0.12 0.061 0.049 0.026 0.096 0.049 0.039 0.056 0.04 0.02 0.013 0.023 0.0078 1 0.014 0.097 0.098 0.093 0.052 0.065 0.054 0.068 0.034 0.038 0.053 0.019 0.018 0.052 0.038 0.027 0.018 0.05 0.015 2 0.06 0.16 0.15 0.054 0.033 0.053 0.046 0.073 0.032 0.014 0.06 0.03 0.017 0.055 0.031 0.016 0.014 0.027 0.0097 3 0.011 0.034 0.021 0.053 0.038 0.038 0.08 0.052 0.1 0.049 0.075 0.079 0.037 0.052 0.053 0.041 0.026 0.04 0.017 4 0.016 0.067 0.099 0.046 0.05 0.042 0.051 0.028 0.03 0.018 0.048 0.022 0.029 0.038 0.035 0.047 0.031 0.08 0.027 Survival curves and confidence intervals \u00b6 XBGSEKaplanTree and XBGSEKaplanNeighbors support estimation of survival curves and confidence intervals via the Exponential Greenwood formula out-of-the-box via the return_ci argument: # fitting xgbse model xgbse_model = XGBSEKaplanNeighbors ( n_neighbors = 50 ) xgbse_model . fit ( X_train , y_train , time_bins = TIME_BINS ) # predicting mean , upper_ci , lower_ci = xgbse_model . predict ( X_valid , return_ci = True ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci ) XGBSEDebiasedBCE does not support estimation of confidence intervals out-of-the-box, but we provide the XGBSEBootstrapEstimator to get non-parametric confidence intervals. As the stacked logistic regressions are trained with more samples (in comparison to neighbor-sets in XGBSEKaplanNeighbors ), confidence intervals are more concentrated: # base model as BCE base_model = XGBSEDebiasedBCE ( PARAMS_XGB_AFT , PARAMS_LR ) # bootstrap meta estimator bootstrap_estimator = XGBSEBootstrapEstimator ( base_model , n_estimators = 20 ) # fitting the meta estimator bootstrap_estimator . fit ( X_train , y_train , validation_data = ( X_valid , y_valid ), early_stopping_rounds = 10 , time_bins = TIME_BINS , ) # predicting mean , upper_ci , lower_ci = bootstrap_estimator . predict ( X_valid , return_ci = True ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci ) The bootstrap abstraction can be used for XBGSEKaplanTree and XBGSEKaplanNeighbors as well, however, the confidence interval will be estimated via bootstrap only (not Exponential Greenwood formula): # base model base_model = XGBSEKaplanTree ( PARAMS_TREE ) # bootstrap meta estimator bootstrap_estimator = XGBSEBootstrapEstimator ( base_model , n_estimators = 100 ) # fitting the meta estimator bootstrap_estimator . fit ( X_train , y_train , time_bins = TIME_BINS , ) # predicting mean , upper_ci , lower_ci = bootstrap_estimator . predict ( X_valid , return_ci = True ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci ) With a sufficiently large n_estimators , interval width shouldn't be much different, with the added benefit of model stability and improved accuracy. Addittionaly, XGBSEBootstrapEstimator allows building confidence intervals for interval probabilities (which is not supported for Exponential Greenwood): # predicting mean , upper_ci , lower_ci = bootstrap_estimator . predict ( X_valid , return_ci = True , return_interval_probs = True ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci ) The parameter ci_width controls the width of the confidence interval. For XGBSEKaplanTree it should be passed at .fit() , as KM curves are pre-calculated for each leaf at fit time to avoid storing training data. # fitting xgbse model xgbse_model = XGBSEKaplanTree ( PARAMS_TREE ) xgbse_model . fit ( X_train , y_train , time_bins = TIME_BINS , ci_width = 0.99 ) # predicting mean , upper_ci , lower_ci = xgbse_model . predict ( X_valid , return_ci = True ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci ) For other models ( XGBSEKaplanNeighbors and XGBSEBootstrapEstimator ) it should be passed at .predict() . # base model model = XGBSEKaplanNeighbors ( PARAMS_XGB_AFT , N_NEIGHBORS ) # fitting the meta estimator model . fit ( X_train , y_train , validation_data = ( X_valid , y_valid ), early_stopping_rounds = 10 , time_bins = TIME_BINS ) # predicting mean , upper_ci , lower_ci = model . predict ( X_valid , return_ci = True , ci_width = 0.99 ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci ) Early stopping \u00b6 A simple interface to xgboost early stopping is provided. # splitting between train, and validation ( X_train , X_valid , y_train , y_valid ) = \\ train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # fitting with early stopping xgb_model = XGBEmbedBCE () xgb_model . fit ( X_train , y_train , validation_data = ( X_valid , y_valid ), early_stopping_rounds = 10 , verbose_eval = 50 ) [0] validation-aft-nloglik:16.86713 Will train until validation-aft-nloglik hasn't improved in 10 rounds. [50] validation-aft-nloglik:3.64540 [100] validation-aft-nloglik:3.53679 [150] validation-aft-nloglik:3.53207 Stopping. Best iteration: [174] validation-aft-nloglik:3.53004 Explainability through prototypes \u00b6 xgbse also provides explainability through prototypes, searching the embedding for neighbors. The idea is to explain model predictions with real samples, providing solid ground to justify them (see [8]). The method .get_neighbors() searches for the n_neighbors nearest neighbors in index_data for each sample in query_data : neighbors = xgb_model . get_neighbors ( query_data = X_valid , index_data = X_train , n_neighbors = 10 ) neighbors . head ( 5 ) index neighbor_1 neighbor_2 neighbor_3 neighbor_4 neighbor_5 neighbor_6 neighbor_7 neighbor_8 neighbor_9 neighbor_10 1225 1151 1513 1200 146 215 452 1284 1127 1895 257 111 1897 1090 1743 1224 892 1695 1624 1546 1418 4 554 9 627 1257 1460 1031 1575 1557 440 1236 858 526 726 1042 177 1640 242 1529 234 1800 399 1431 1313 205 1738 599 954 1694 1715 1651 828 541 992 This way, we can inspect neighbors of a given sample to try to explain predictions. For instance, we can choose a reference and check that its neighbors actually are very similar as a sanity check: i = 0 reference = X_valid . iloc [ i ] reference . name = 'reference' train_neighs = X_train . loc [ neighbors . iloc [ i ]] pd . concat ([ reference . to_frame () . T , train_neighs ]) index x0 x1 x2 x3 x4 x5 x6 x7 x8 reference 5.7 5.7 11 5.6 1 1 0 1 86 1151 5.8 5.9 11 5.5 1 1 0 1 82 1513 5.5 5.5 11 5.6 1 1 0 1 79 1200 5.7 6 11 5.6 1 1 0 1 76 146 5.9 5.9 11 5.5 0 1 0 1 75 215 5.8 5.5 11 5.4 1 1 0 1 78 452 5.7 5.7 12 5.5 0 0 0 1 76 1284 5.6 6.2 11 5.6 1 0 0 1 79 1127 5.5 5.1 11 5.5 1 1 0 1 86 1895 5.5 5.4 10 5.5 1 1 0 1 85 257 5.7 6 9.6 5.6 1 1 0 1 76 We also can compare the Kaplan-Meier curve estimated from the neighbors to the actual model prediction, checking that it is inside the confidence interval: from xgbse.non_parametric import calculate_kaplan_vectorized mean , high , low = calculate_kaplan_vectorized ( np . array ([ y [ 'c2' ][ neighbors . iloc [ i ]]]), np . array ([ y [ 'c1' ][ neighbors . iloc [ i ]]]), TIME_BINS ) model_surv = xgb_model . predict ( X_valid ) plt . figure ( figsize = ( 12 , 4 ), dpi = 120 ) plt . plot ( model_surv . columns , model_surv . iloc [ i ]) plt . plot ( mean . columns , mean . iloc [ 0 ]) plt . fill_between ( mean . columns , low . iloc [ 0 ], high . iloc [ 0 ], alpha = 0.1 , color = 'red' ) Specifically, for XBGSEKaplanNeighbors prototype predictions and model predictions should match exactly if n_neighbors is the same and query_data is equal to the training data. Metrics \u00b6 We made our own metrics submodule to make the lib self-contained. xgbse.metrics implements C-index, Brier Score and D-Calibration from [9], including adaptations to deal with censoring: # training model xgbse_model = XGBSEKaplanNeighbors ( PARAMS_XGB_AFT , n_neighbors = 30 ) xgbse_model . fit ( X_train , y_train , validation_data = ( X_valid , y_valid ), early_stopping_rounds = 10 , time_bins = TIME_BINS ) # predicting preds = xgbse_model . predict ( X_valid ) # importing metrics from xgbse.metrics import ( concordance_index , approx_brier_score , dist_calibration_score ) # running metrics print ( f 'C-index: { concordance_index ( y_valid , preds ) } ' ) print ( f 'Avg. Brier Score: { approx_brier_score ( y_valid , preds ) } ' ) print ( f \"\"\"D-Calibration: { dist_calibration_score ( y_valid , preds ) > 0.05 } \"\"\" ) C-index: 0.6495863029409356 Avg. Brier Score: 0.1704190044350422 D-Calibration: True As metrics follow the score_func(y, y_pred, **kwargs) pattern, we can use the sklearn model selection module easily: from sklearn.model_selection import cross_val_score from sklearn.metrics import make_scorer xgbse_model = XGBSEKaplanTree ( PARAMS_TREE ) results = cross_val_score ( xgbse_model , X , y , scoring = make_scorer ( approx_brier_score )) results array([0.17432953, 0.15907712, 0.13783666, 0.16770409, 0.16792016]) References \u00b6 [1] Practical Lessons from Predicting Clicks on Ads at Facebook : paper that shows how stacking boosting models with logistic regression improves performance and calibration [2] Feature transformations with ensembles of trees : scikit-learn post showing tree ensembles as feature transformers [3] Calibration of probabilities for tree-based models : blog post showing a practical example of tree ensemble probability calibration with a logistic regression [4] Supervised dimensionality reduction and clustering at scale with RFs with UMAP : blog post showing how forests of decision trees act as noise filters, reducing intrinsic dimension of the dataset. [5] Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors : inspiration for the BCE method (multi-task logistic regression) [6] The Brier Score under Administrative Censoring: Problems and Solutions : reference to BCE (binary cross-entropy survival method). [7] The Greenwood and Exponential Greenwood Confidence Intervals in Survival Analysis : reference we used for the Exponential Greenwood formula from KM confidence intervals [8] Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable : paper showing a very similar method for extracting prototypes [9] Effective Ways to Build and Evaluate Individual Survival Distributions : paper showing how to validate survival analysis models with different metrics Citing xgbse \u00b6 To cite this repository: @software{xgbse2020github, author = {Davi Vieira and Gabriel Gimenez and Guilherme Marmerola and Vitor Estima}, title = {XGBoost Survival Embeddings: improving statistical properties of XGBoost survival analysis implementation}, url = {http://github.com/loft-br/xgboost-survival-embeddings}, version = {0.1.0}, year = {2020}, }","title":"Home"},{"location":"index.html#xgbse-xgboost-survival-embeddings","text":"\"There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.\" - Leo Breiman, Statistical Modeling: The Two Cultures Survival Analysis is a powerful statistical technique with a wide range of applications such as predictive maintenance, customer churn, credit risk, asset liquidity risk, and others. However, it has not yet seen widespread adoption in industry, with most implementations embracing one of two cultures: models with sound statistical properties, but lacking in expressivess and computational efficiency highly efficient and expressive models, but lacking in statistical rigor xgbse aims to unite the two cultures in a single package, adding a layer of statistical rigor to the highly expressive and computationally effcient xgboost survival analysis implementation. The package offers: calibrated and unbiased survival curves with confidence intervals (instead of point predictions) great predictive power, competitive to vanilla xgboost efficient, easy to use implementation explainability through prototypes This is a research project by Loft Data Science Team , however we invite the community to contribute. Please help by trying it out, reporting bugs, and letting us know what you think!","title":"xgbse: XGBoost Survival Embeddings"},{"location":"index.html#todo-installation","text":"...","title":"[TODO] Installation"},{"location":"index.html#usage","text":"","title":"Usage"},{"location":"index.html#basic-usage","text":"The package follows scikit-learn API, with a minor adaptation to work with time and event data ( y as a numpy structured array of times and events). .predict() returns a dataframe where each column is a time window and values represent the probability of survival before or exactly at the time window. # importing dataset from pycox package from pycox.datasets import metabric # importing model and utils from xgbse from xgbse import XGBEmbedKaplanNeighbors from xgbse.converters import convert_to_structured # getting data df = metabric . read_df () # splitting to X, y format X = df . drop ([ 'duration' , 'event' ], axis = 1 ) y = convert_to_structured ( df [ 'duration' ], df [ 'event' ]) # fitting xgbse model xgbse_model = XGBEmbedKaplanNeighbors ( n_neighbors = 50 ) xgbse_model . fit ( X , y ) # predicting event_probs = xgbse_model . predict ( X ) event_probs . head () index 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 0 0.98 0.87 0.81 0.74 0.71 0.66 0.53 0.47 0.42 0.4 0.3 0.25 0.21 0.16 0.12 0.098 0.085 0.062 0.054 1 0.99 0.89 0.79 0.7 0.64 0.58 0.53 0.46 0.42 0.39 0.33 0.31 0.3 0.24 0.21 0.18 0.16 0.11 0.095 2 0.94 0.78 0.63 0.57 0.54 0.49 0.44 0.37 0.34 0.32 0.26 0.23 0.21 0.16 0.13 0.11 0.098 0.072 0.062 3 0.99 0.95 0.93 0.88 0.84 0.81 0.73 0.67 0.57 0.52 0.45 0.37 0.33 0.28 0.23 0.19 0.16 0.12 0.1 4 0.98 0.92 0.82 0.77 0.72 0.68 0.63 0.6 0.57 0.55 0.51 0.48 0.45 0.42 0.38 0.33 0.3 0.22 0.2 You can also get interval predictions (probability of failing exactly at each time window) using return_interval_probs : # point predictions interval_probs = xgbse_model . predict ( X_valid , return_interval_probs = True ) interval_probs . head () index 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 0 0.024 0.1 0.058 0.07 0.034 0.05 0.12 0.061 0.049 0.026 0.096 0.049 0.039 0.056 0.04 0.02 0.013 0.023 0.0078 1 0.014 0.097 0.098 0.093 0.052 0.065 0.054 0.068 0.034 0.038 0.053 0.019 0.018 0.052 0.038 0.027 0.018 0.05 0.015 2 0.06 0.16 0.15 0.054 0.033 0.053 0.046 0.073 0.032 0.014 0.06 0.03 0.017 0.055 0.031 0.016 0.014 0.027 0.0097 3 0.011 0.034 0.021 0.053 0.038 0.038 0.08 0.052 0.1 0.049 0.075 0.079 0.037 0.052 0.053 0.041 0.026 0.04 0.017 4 0.016 0.067 0.099 0.046 0.05 0.042 0.051 0.028 0.03 0.018 0.048 0.022 0.029 0.038 0.035 0.047 0.031 0.08 0.027","title":"Basic usage"},{"location":"index.html#survival-curves-and-confidence-intervals","text":"XBGSEKaplanTree and XBGSEKaplanNeighbors support estimation of survival curves and confidence intervals via the Exponential Greenwood formula out-of-the-box via the return_ci argument: # fitting xgbse model xgbse_model = XGBSEKaplanNeighbors ( n_neighbors = 50 ) xgbse_model . fit ( X_train , y_train , time_bins = TIME_BINS ) # predicting mean , upper_ci , lower_ci = xgbse_model . predict ( X_valid , return_ci = True ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci ) XGBSEDebiasedBCE does not support estimation of confidence intervals out-of-the-box, but we provide the XGBSEBootstrapEstimator to get non-parametric confidence intervals. As the stacked logistic regressions are trained with more samples (in comparison to neighbor-sets in XGBSEKaplanNeighbors ), confidence intervals are more concentrated: # base model as BCE base_model = XGBSEDebiasedBCE ( PARAMS_XGB_AFT , PARAMS_LR ) # bootstrap meta estimator bootstrap_estimator = XGBSEBootstrapEstimator ( base_model , n_estimators = 20 ) # fitting the meta estimator bootstrap_estimator . fit ( X_train , y_train , validation_data = ( X_valid , y_valid ), early_stopping_rounds = 10 , time_bins = TIME_BINS , ) # predicting mean , upper_ci , lower_ci = bootstrap_estimator . predict ( X_valid , return_ci = True ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci ) The bootstrap abstraction can be used for XBGSEKaplanTree and XBGSEKaplanNeighbors as well, however, the confidence interval will be estimated via bootstrap only (not Exponential Greenwood formula): # base model base_model = XGBSEKaplanTree ( PARAMS_TREE ) # bootstrap meta estimator bootstrap_estimator = XGBSEBootstrapEstimator ( base_model , n_estimators = 100 ) # fitting the meta estimator bootstrap_estimator . fit ( X_train , y_train , time_bins = TIME_BINS , ) # predicting mean , upper_ci , lower_ci = bootstrap_estimator . predict ( X_valid , return_ci = True ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci ) With a sufficiently large n_estimators , interval width shouldn't be much different, with the added benefit of model stability and improved accuracy. Addittionaly, XGBSEBootstrapEstimator allows building confidence intervals for interval probabilities (which is not supported for Exponential Greenwood): # predicting mean , upper_ci , lower_ci = bootstrap_estimator . predict ( X_valid , return_ci = True , return_interval_probs = True ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci ) The parameter ci_width controls the width of the confidence interval. For XGBSEKaplanTree it should be passed at .fit() , as KM curves are pre-calculated for each leaf at fit time to avoid storing training data. # fitting xgbse model xgbse_model = XGBSEKaplanTree ( PARAMS_TREE ) xgbse_model . fit ( X_train , y_train , time_bins = TIME_BINS , ci_width = 0.99 ) # predicting mean , upper_ci , lower_ci = xgbse_model . predict ( X_valid , return_ci = True ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci ) For other models ( XGBSEKaplanNeighbors and XGBSEBootstrapEstimator ) it should be passed at .predict() . # base model model = XGBSEKaplanNeighbors ( PARAMS_XGB_AFT , N_NEIGHBORS ) # fitting the meta estimator model . fit ( X_train , y_train , validation_data = ( X_valid , y_valid ), early_stopping_rounds = 10 , time_bins = TIME_BINS ) # predicting mean , upper_ci , lower_ci = model . predict ( X_valid , return_ci = True , ci_width = 0.99 ) # plotting CIs plot_ci ( mean , upper_ci , lower_ci )","title":"Survival curves and confidence intervals"},{"location":"index.html#early-stopping","text":"A simple interface to xgboost early stopping is provided. # splitting between train, and validation ( X_train , X_valid , y_train , y_valid ) = \\ train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # fitting with early stopping xgb_model = XGBEmbedBCE () xgb_model . fit ( X_train , y_train , validation_data = ( X_valid , y_valid ), early_stopping_rounds = 10 , verbose_eval = 50 ) [0] validation-aft-nloglik:16.86713 Will train until validation-aft-nloglik hasn't improved in 10 rounds. [50] validation-aft-nloglik:3.64540 [100] validation-aft-nloglik:3.53679 [150] validation-aft-nloglik:3.53207 Stopping. Best iteration: [174] validation-aft-nloglik:3.53004","title":"Early stopping"},{"location":"index.html#explainability-through-prototypes","text":"xgbse also provides explainability through prototypes, searching the embedding for neighbors. The idea is to explain model predictions with real samples, providing solid ground to justify them (see [8]). The method .get_neighbors() searches for the n_neighbors nearest neighbors in index_data for each sample in query_data : neighbors = xgb_model . get_neighbors ( query_data = X_valid , index_data = X_train , n_neighbors = 10 ) neighbors . head ( 5 ) index neighbor_1 neighbor_2 neighbor_3 neighbor_4 neighbor_5 neighbor_6 neighbor_7 neighbor_8 neighbor_9 neighbor_10 1225 1151 1513 1200 146 215 452 1284 1127 1895 257 111 1897 1090 1743 1224 892 1695 1624 1546 1418 4 554 9 627 1257 1460 1031 1575 1557 440 1236 858 526 726 1042 177 1640 242 1529 234 1800 399 1431 1313 205 1738 599 954 1694 1715 1651 828 541 992 This way, we can inspect neighbors of a given sample to try to explain predictions. For instance, we can choose a reference and check that its neighbors actually are very similar as a sanity check: i = 0 reference = X_valid . iloc [ i ] reference . name = 'reference' train_neighs = X_train . loc [ neighbors . iloc [ i ]] pd . concat ([ reference . to_frame () . T , train_neighs ]) index x0 x1 x2 x3 x4 x5 x6 x7 x8 reference 5.7 5.7 11 5.6 1 1 0 1 86 1151 5.8 5.9 11 5.5 1 1 0 1 82 1513 5.5 5.5 11 5.6 1 1 0 1 79 1200 5.7 6 11 5.6 1 1 0 1 76 146 5.9 5.9 11 5.5 0 1 0 1 75 215 5.8 5.5 11 5.4 1 1 0 1 78 452 5.7 5.7 12 5.5 0 0 0 1 76 1284 5.6 6.2 11 5.6 1 0 0 1 79 1127 5.5 5.1 11 5.5 1 1 0 1 86 1895 5.5 5.4 10 5.5 1 1 0 1 85 257 5.7 6 9.6 5.6 1 1 0 1 76 We also can compare the Kaplan-Meier curve estimated from the neighbors to the actual model prediction, checking that it is inside the confidence interval: from xgbse.non_parametric import calculate_kaplan_vectorized mean , high , low = calculate_kaplan_vectorized ( np . array ([ y [ 'c2' ][ neighbors . iloc [ i ]]]), np . array ([ y [ 'c1' ][ neighbors . iloc [ i ]]]), TIME_BINS ) model_surv = xgb_model . predict ( X_valid ) plt . figure ( figsize = ( 12 , 4 ), dpi = 120 ) plt . plot ( model_surv . columns , model_surv . iloc [ i ]) plt . plot ( mean . columns , mean . iloc [ 0 ]) plt . fill_between ( mean . columns , low . iloc [ 0 ], high . iloc [ 0 ], alpha = 0.1 , color = 'red' ) Specifically, for XBGSEKaplanNeighbors prototype predictions and model predictions should match exactly if n_neighbors is the same and query_data is equal to the training data.","title":"Explainability through prototypes"},{"location":"index.html#metrics","text":"We made our own metrics submodule to make the lib self-contained. xgbse.metrics implements C-index, Brier Score and D-Calibration from [9], including adaptations to deal with censoring: # training model xgbse_model = XGBSEKaplanNeighbors ( PARAMS_XGB_AFT , n_neighbors = 30 ) xgbse_model . fit ( X_train , y_train , validation_data = ( X_valid , y_valid ), early_stopping_rounds = 10 , time_bins = TIME_BINS ) # predicting preds = xgbse_model . predict ( X_valid ) # importing metrics from xgbse.metrics import ( concordance_index , approx_brier_score , dist_calibration_score ) # running metrics print ( f 'C-index: { concordance_index ( y_valid , preds ) } ' ) print ( f 'Avg. Brier Score: { approx_brier_score ( y_valid , preds ) } ' ) print ( f \"\"\"D-Calibration: { dist_calibration_score ( y_valid , preds ) > 0.05 } \"\"\" ) C-index: 0.6495863029409356 Avg. Brier Score: 0.1704190044350422 D-Calibration: True As metrics follow the score_func(y, y_pred, **kwargs) pattern, we can use the sklearn model selection module easily: from sklearn.model_selection import cross_val_score from sklearn.metrics import make_scorer xgbse_model = XGBSEKaplanTree ( PARAMS_TREE ) results = cross_val_score ( xgbse_model , X , y , scoring = make_scorer ( approx_brier_score )) results array([0.17432953, 0.15907712, 0.13783666, 0.16770409, 0.16792016])","title":"Metrics"},{"location":"index.html#references","text":"[1] Practical Lessons from Predicting Clicks on Ads at Facebook : paper that shows how stacking boosting models with logistic regression improves performance and calibration [2] Feature transformations with ensembles of trees : scikit-learn post showing tree ensembles as feature transformers [3] Calibration of probabilities for tree-based models : blog post showing a practical example of tree ensemble probability calibration with a logistic regression [4] Supervised dimensionality reduction and clustering at scale with RFs with UMAP : blog post showing how forests of decision trees act as noise filters, reducing intrinsic dimension of the dataset. [5] Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors : inspiration for the BCE method (multi-task logistic regression) [6] The Brier Score under Administrative Censoring: Problems and Solutions : reference to BCE (binary cross-entropy survival method). [7] The Greenwood and Exponential Greenwood Confidence Intervals in Survival Analysis : reference we used for the Exponential Greenwood formula from KM confidence intervals [8] Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable : paper showing a very similar method for extracting prototypes [9] Effective Ways to Build and Evaluate Individual Survival Distributions : paper showing how to validate survival analysis models with different metrics","title":"References"},{"location":"index.html#citing-xgbse","text":"To cite this repository: @software{xgbse2020github, author = {Davi Vieira and Gabriel Gimenez and Guilherme Marmerola and Vitor Estima}, title = {XGBoost Survival Embeddings: improving statistical properties of XGBoost survival analysis implementation}, url = {http://github.com/loft-br/xgboost-survival-embeddings}, version = {0.1.0}, year = {2020}, }","title":"Citing xgbse"},{"location":"base.html","text":"\u00b6 Base class for all estimators in xgbse get_neighbors(self, query_data, index_data=None, query_id=None, index_id=None, n_neighbors=30) \u00b6 Get survival comparables (size: n_neighbors) for each unit in a dataframe X. If units array is specified, comparables will be returned using its identifiers. If not, a dataframe of comparables indexes for each sample in X is returned. Parameters: query_data ( pd.DataFrame ) \u2013 Dataframe of features to be used as input query_id ( [pd.Series, np.array] ) \u2013 Series or array of identification for each sample of query_data. Will be used in set_index if specified. index_id ( [pd.Series, np.array] ) \u2013 Series or array of identification for each sample of index_id. If specified, comparables will be returned using this identifier. n_neighbors ( int ) \u2013 Number of neighbors/comparables to be considered. Returns: comps_df (pd.DataFrame) \u2013 A dataframe of comparables/neighbors for each evaluated sample. If units identifier is specified, the output dataframe is converted to use units the proper identifier for each sample. The reference sample is considered to be the index of the dataframe and its comparables are its specific row values.","title":"Base"},{"location":"base.html#xgbse._base.XGBSEBaseEstimator","text":"Base class for all estimators in xgbse","title":"xgbse._base.XGBSEBaseEstimator"},{"location":"base.html#xgbse._base.XGBSEBaseEstimator.get_neighbors","text":"Get survival comparables (size: n_neighbors) for each unit in a dataframe X. If units array is specified, comparables will be returned using its identifiers. If not, a dataframe of comparables indexes for each sample in X is returned. Parameters: query_data ( pd.DataFrame ) \u2013 Dataframe of features to be used as input query_id ( [pd.Series, np.array] ) \u2013 Series or array of identification for each sample of query_data. Will be used in set_index if specified. index_id ( [pd.Series, np.array] ) \u2013 Series or array of identification for each sample of index_id. If specified, comparables will be returned using this identifier. n_neighbors ( int ) \u2013 Number of neighbors/comparables to be considered. Returns: comps_df (pd.DataFrame) \u2013 A dataframe of comparables/neighbors for each evaluated sample. If units identifier is specified, the output dataframe is converted to use units the proper identifier for each sample. The reference sample is considered to be the index of the dataframe and its comparables are its specific row values.","title":"get_neighbors()"},{"location":"bce.html","text":"\u00b6 Class to train a set of logistic regressions on top of the embedding produced by xgboost models. Each logistic regression predicts survival at different user-defined discrete time windows. The classifiers remove individuals as they are censored, with targets that are indicators of surviving at each window. Adapted from source: http://quinonero.net/Publications/predicting-clicks-facebook.pdf __init__(self, xgb_params={'objective': 'survival:aft', 'eval_metric': 'aft-nloglik', 'aft_loss_distribution': 'normal', 'aft_loss_distribution_scale': 1, 'tree_method': 'hist', 'learning_rate': 0.05, 'max_depth': 8, 'booster': 'dart', 'subsample': 0.5, 'min_child_weight': 50, 'colsample_bynode': 0.5}, lr_params={'C': 0.001, 'max_iter': 500}, n_jobs=-1) special \u00b6 Construct XGBSEDebiasedBCE instance Parameters: xgb_params ( Dict ) \u2013 parameters for XGBoost model, see https://xgboost.readthedocs.io/en/latest/parameter.html lr_params ( Dict ) \u2013 parameters for Logistic Regression model, see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html n_jobs ( Int ) \u2013 Number of CPU cores used to fit logistic regressions via joblib. fit(self, X, y, num_boost_round=1000, validation_data=None, early_stopping_rounds=None, verbose_eval=0, persist_train=False, index_id=None, time_bins=None) \u00b6 Transform feature space by fitting a XGBoost model and outputting its leaf indices. Leaves are transformed and considered as dummy variables to fit multiple logistic regression models to each evaluated time bin. Parameters: X ( [pd.DataFrame, np.array] ) \u2013 features to be used while fitting XGBoost model y ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. num_boost_round ( Int ) \u2013 Number of boosting iterations. validation_data ( Tuple ) \u2013 Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping early_stopping_rounds ( Int ) \u2013 Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation. verbose_eval ( [Bool, Int] ) \u2013 level of verbosity. See xgboost.train documentation. persist_train ( Bool ) \u2013 whether or not to persist training data to use explainability through prototypes index_id ( pd.Index ) \u2013 user defined index if intended to use explainability through prototypes time_bins ( np.array ) \u2013 specified time windows to use when making survival predictions Returns: XGBSEDebiasedBCE \u2013 Trained XGBSEDebiasedBCE instance predict(self, X, return_interval_probs=False) \u00b6 Predicts survival probabilities using the XGBoost + Logistic Regression pipeline. Parameters: X ( pd.DataFrame ) \u2013 Dataframe of features to be used as input for the XGBoost model. return_interval_probs ( Bool ) \u2013 Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. Default is False. Returns: pd.DataFrame \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities.","title":"BCE"},{"location":"bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE","text":"Class to train a set of logistic regressions on top of the embedding produced by xgboost models. Each logistic regression predicts survival at different user-defined discrete time windows. The classifiers remove individuals as they are censored, with targets that are indicators of surviving at each window. Adapted from source: http://quinonero.net/Publications/predicting-clicks-facebook.pdf","title":"xgbse._debiased_bce.XGBSEDebiasedBCE"},{"location":"bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.__init__","text":"Construct XGBSEDebiasedBCE instance Parameters: xgb_params ( Dict ) \u2013 parameters for XGBoost model, see https://xgboost.readthedocs.io/en/latest/parameter.html lr_params ( Dict ) \u2013 parameters for Logistic Regression model, see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html n_jobs ( Int ) \u2013 Number of CPU cores used to fit logistic regressions via joblib.","title":"__init__()"},{"location":"bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.fit","text":"Transform feature space by fitting a XGBoost model and outputting its leaf indices. Leaves are transformed and considered as dummy variables to fit multiple logistic regression models to each evaluated time bin. Parameters: X ( [pd.DataFrame, np.array] ) \u2013 features to be used while fitting XGBoost model y ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. num_boost_round ( Int ) \u2013 Number of boosting iterations. validation_data ( Tuple ) \u2013 Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping early_stopping_rounds ( Int ) \u2013 Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation. verbose_eval ( [Bool, Int] ) \u2013 level of verbosity. See xgboost.train documentation. persist_train ( Bool ) \u2013 whether or not to persist training data to use explainability through prototypes index_id ( pd.Index ) \u2013 user defined index if intended to use explainability through prototypes time_bins ( np.array ) \u2013 specified time windows to use when making survival predictions Returns: XGBSEDebiasedBCE \u2013 Trained XGBSEDebiasedBCE instance","title":"fit()"},{"location":"bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.predict","text":"Predicts survival probabilities using the XGBoost + Logistic Regression pipeline. Parameters: X ( pd.DataFrame ) \u2013 Dataframe of features to be used as input for the XGBoost model. return_interval_probs ( Bool ) \u2013 Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. Default is False. Returns: pd.DataFrame \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities.","title":"predict()"},{"location":"converters.html","text":"\u00b6 Converts data in T, E format to structured numpy array so we can use integrated_brier_score functions from sksurv Parameters: T ( np.array ) \u2013 array of times E ( np.array ) \u2013 array of events Returns: np.array \u2013 structured array containing the boolean event indicator as first field, and time of event or time of censoring as second field \u00b6 convert structured array y into event indicator and time of event Parameters: y ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. Returns: T ([np.array, pd.Series]) \u2013 time of events E ([np.array, pd.Series]): binary event indicator","title":"Converters"},{"location":"converters.html#xgbse.converters.convert_to_structured","text":"Converts data in T, E format to structured numpy array so we can use integrated_brier_score functions from sksurv Parameters: T ( np.array ) \u2013 array of times E ( np.array ) \u2013 array of events Returns: np.array \u2013 structured array containing the boolean event indicator as first field, and time of event or time of censoring as second field","title":"xgbse.converters.convert_to_structured"},{"location":"converters.html#xgbse.converters.convert_y","text":"convert structured array y into event indicator and time of event Parameters: y ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. Returns: T ([np.array, pd.Series]) \u2013 time of events E ([np.array, pd.Series]): binary event indicator","title":"xgbse.converters.convert_y"},{"location":"extrapolation.html","text":"\u00b6 extrapolate_constant_risk(survival, final_time, n_windows, lags=-1) \u00b6 Extrapolate a survival curve assuming constant risk. Parameters: survival ( pd.DataFrame ) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). final_time ( Float ) \u2013 final time for extrapolation n_windows ( Int ) \u2013 number of time windows to compute from last time window in survival to final_time lags ( Int ) \u2013 lags to compute constant risk. if negative, will use the last \"lags\" values if positive, will remove the first \"lags\" values if 0, will use all values Returns: pd.DataFrame \u2013 survival dataset with appended extrapolated windows","title":"Extrapolation"},{"location":"extrapolation.html#xgbse.extrapolation","text":"","title":"xgbse.extrapolation"},{"location":"extrapolation.html#xgbse.extrapolation.extrapolate_constant_risk","text":"Extrapolate a survival curve assuming constant risk. Parameters: survival ( pd.DataFrame ) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). final_time ( Float ) \u2013 final time for extrapolation n_windows ( Int ) \u2013 number of time windows to compute from last time window in survival to final_time lags ( Int ) \u2013 lags to compute constant risk. if negative, will use the last \"lags\" values if positive, will remove the first \"lags\" values if 0, will use all values Returns: pd.DataFrame \u2013 survival dataset with appended extrapolated windows","title":"extrapolate_constant_risk()"},{"location":"kaplan.html","text":"\u00b6 XGBSEKaplanNeighbor \u00b6 Convert xgboost into a nearest neighbor model, where we use hamming distance to define similar elements as the ones that co-ocurred the most at the ensemble terminal nodes. Then, at each neighbor-set compute survival estimates with the Kaplan-Meier estimator. __init__(self, xgb_params={'objective': 'survival:aft', 'eval_metric': 'aft-nloglik', 'aft_loss_distribution': 'normal', 'aft_loss_distribution_scale': 1, 'tree_method': 'hist', 'learning_rate': 0.05, 'max_depth': 8, 'booster': 'dart', 'subsample': 0.5, 'min_child_weight': 50, 'colsample_bynode': 0.5}, n_neighbors=30, radius=None) special \u00b6 Parameters: xgb_params ( Dict ) \u2013 parameters for XGBoost model, see https://xgboost.readthedocs.io/en/latest/parameter.html n_neighbors ( Int ) \u2013 number of neighbors for computing KM estimates radius ( Float ) \u2013 If set, uses a radius around the point for neighbors search fit(self, X, y, num_boost_round=1000, validation_data=None, early_stopping_rounds=None, verbose_eval=0, persist_train=True, index_id=None, time_bins=None) \u00b6 Transform feature space by fitting a XGBoost model and outputting its leaf indices. Build search index in the new space to allow nearest neighbor queries at scoring time. Parameters: X ( [pd.DataFrame, np.array] ) \u2013 design matrix to fit XGBoost model y ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. num_boost_round ( Int ) \u2013 Number of boosting iterations. validation_data ( Tuple ) \u2013 Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping early_stopping_rounds ( Int ) \u2013 Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation. verbose_eval ( [Bool, Int] ) \u2013 level of verbosity. See xgboost.train documentation. persist_train ( Bool ) \u2013 whether or not to persist training data to use explainability through prototypes index_id ( pd.Index ) \u2013 user defined index if intended to use explainability through prototypes time_bins ( np.array ) \u2013 specified time windows to use when making survival predictions Returns: XGBSEKaplanNeighbors \u2013 fitted instance of XGBSEKaplanNeighbors predict(self, X, time_bins=None, return_ci=False, ci_width=0.683, return_interval_probs=False) \u00b6 Make queries to nearest neighbor search index build on the transformed XGBoost space. Compute a Kaplan-Meier estimator for each neighbor-set. Predict the KM estimators. Parameters: X ( pd.DataFrame ) \u2013 data frame with samples to generate predictions time_bins ( np.array ) \u2013 specified time windows to use when making survival predictions return_ci ( Bool ) \u2013 whether to return confidence intervals via the Exponential Greenwood formula ci_width ( Float ) \u2013 width of confidence interval return_interval_probs ( Bool ) \u2013 Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. Returns: (pd.DataFrame) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci (np.array): upper confidence interval for the survival probability values lower_ci (np.array): lower confidence interval for the survival probability values \u00b6 XGBSEKaplanTree \u00b6 Single tree implementation as a simplification to XGBSEKaplanNeighbors . Instead of doing nearest neighbor searches, fit a single tree via xgboost and calculate KM curves at each of its leaves. fit(self, X, y, persist_train=True, index_id=None, time_bins=None, ci_width=0.683, **xgb_kwargs) \u00b6 Fit a single decision tree using xgboost. For each leaf in the tree, build a Kaplan-Meier estimator. Args: X ([ pd . DataFrame , np . array ]): design matrix to fit XGBoost model y ( structured array ( numpy . bool_ , numpy . number )): binary event indicator as first field , and time of event or time of censoring as second field . persist_train ( Bool ): whether or not to persist training data to use explainability through prototypes index_id ( pd . Index ): user defined index if intended to use explainability through prototypes time_bins ( np . array ): specified time windows to use when making survival predictions ci_width ( Float ): width of confidence interval Returns: XGBSEKaplanTree \u2013 Trained instance of XGBSEKaplanTree predict(self, X, return_ci=False, return_interval_probs=False) \u00b6 Run samples through tree until terminal nodes. Predict the Kaplan-Meier estimator associated to the leaf node each sample ended into. Parameters: X ( pd.DataFrame ) \u2013 data frame with samples to generate predictions return_ci ( Bool ) \u2013 whether to return confidence intervals via the Exponential Greenwood formula return_interval_probs ( Bool ) \u2013 Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. Returns: preds_df (pd.DataFrame) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci (np.array): upper confidence interval for the survival probability values lower_ci (np.array): lower confidence interval for the survival probability values","title":"Kaplan"},{"location":"kaplan.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors","text":"","title":"xgbse._kaplan_neighbors.XGBSEKaplanNeighbors"},{"location":"kaplan.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors--xgbsekaplanneighbor","text":"Convert xgboost into a nearest neighbor model, where we use hamming distance to define similar elements as the ones that co-ocurred the most at the ensemble terminal nodes. Then, at each neighbor-set compute survival estimates with the Kaplan-Meier estimator.","title":"XGBSEKaplanNeighbor"},{"location":"kaplan.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.__init__","text":"Parameters: xgb_params ( Dict ) \u2013 parameters for XGBoost model, see https://xgboost.readthedocs.io/en/latest/parameter.html n_neighbors ( Int ) \u2013 number of neighbors for computing KM estimates radius ( Float ) \u2013 If set, uses a radius around the point for neighbors search","title":"__init__()"},{"location":"kaplan.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.fit","text":"Transform feature space by fitting a XGBoost model and outputting its leaf indices. Build search index in the new space to allow nearest neighbor queries at scoring time. Parameters: X ( [pd.DataFrame, np.array] ) \u2013 design matrix to fit XGBoost model y ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. num_boost_round ( Int ) \u2013 Number of boosting iterations. validation_data ( Tuple ) \u2013 Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping early_stopping_rounds ( Int ) \u2013 Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation. verbose_eval ( [Bool, Int] ) \u2013 level of verbosity. See xgboost.train documentation. persist_train ( Bool ) \u2013 whether or not to persist training data to use explainability through prototypes index_id ( pd.Index ) \u2013 user defined index if intended to use explainability through prototypes time_bins ( np.array ) \u2013 specified time windows to use when making survival predictions Returns: XGBSEKaplanNeighbors \u2013 fitted instance of XGBSEKaplanNeighbors","title":"fit()"},{"location":"kaplan.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.predict","text":"Make queries to nearest neighbor search index build on the transformed XGBoost space. Compute a Kaplan-Meier estimator for each neighbor-set. Predict the KM estimators. Parameters: X ( pd.DataFrame ) \u2013 data frame with samples to generate predictions time_bins ( np.array ) \u2013 specified time windows to use when making survival predictions return_ci ( Bool ) \u2013 whether to return confidence intervals via the Exponential Greenwood formula ci_width ( Float ) \u2013 width of confidence interval return_interval_probs ( Bool ) \u2013 Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. Returns: (pd.DataFrame) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci (np.array): upper confidence interval for the survival probability values lower_ci (np.array): lower confidence interval for the survival probability values","title":"predict()"},{"location":"kaplan.html#xgbse._kaplan_neighbors.XGBSEKaplanTree","text":"","title":"xgbse._kaplan_neighbors.XGBSEKaplanTree"},{"location":"kaplan.html#xgbse._kaplan_neighbors.XGBSEKaplanTree--xgbsekaplantree","text":"Single tree implementation as a simplification to XGBSEKaplanNeighbors . Instead of doing nearest neighbor searches, fit a single tree via xgboost and calculate KM curves at each of its leaves.","title":"XGBSEKaplanTree"},{"location":"kaplan.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.fit","text":"Fit a single decision tree using xgboost. For each leaf in the tree, build a Kaplan-Meier estimator. Args: X ([ pd . DataFrame , np . array ]): design matrix to fit XGBoost model y ( structured array ( numpy . bool_ , numpy . number )): binary event indicator as first field , and time of event or time of censoring as second field . persist_train ( Bool ): whether or not to persist training data to use explainability through prototypes index_id ( pd . Index ): user defined index if intended to use explainability through prototypes time_bins ( np . array ): specified time windows to use when making survival predictions ci_width ( Float ): width of confidence interval Returns: XGBSEKaplanTree \u2013 Trained instance of XGBSEKaplanTree","title":"fit()"},{"location":"kaplan.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.predict","text":"Run samples through tree until terminal nodes. Predict the Kaplan-Meier estimator associated to the leaf node each sample ended into. Parameters: X ( pd.DataFrame ) \u2013 data frame with samples to generate predictions return_ci ( Bool ) \u2013 whether to return confidence intervals via the Exponential Greenwood formula return_interval_probs ( Bool ) \u2013 Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. Returns: preds_df (pd.DataFrame) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci (np.array): upper confidence interval for the survival probability values lower_ci (np.array): lower confidence interval for the survival probability values","title":"predict()"},{"location":"meta.html","text":"\u00b6 Bootstrap meta-estimator for XGBSE models: -- allows for confidence interval estimation in BCE -- provides variance stabilization for all models, mainly Kaplan Tree Performs simple bootstrap with sample size equal to training set size. __init__(self, base_estimator, n_estimators=10, random_state=42) special \u00b6 Parameters: base_estimator ( XGBSEBaseEstimator ) \u2013 base estimator for bootstrap procedure n_estimators ( Int ) \u2013 number of estimators to fit in bootstrap procedure random_state ( Int ) \u2013 random state for resampling function fit(self, X, y, **kwargs) \u00b6 Fit several (base) estimators and store them. Parameters: X ( [pd.DataFrame, np.array] ) \u2013 features to be used while fitting XGBoost model y ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. **kwargs (``) \u2013 keyword arguments to be passed to .fit() method of base_estimator Returns: XGBSEBootstrapEstimator \u2013 Trained instance of XGBSEBootstrapEstimator predict(self, X, return_ci=False, ci_width=0.683, return_interval_probs=False) \u00b6 Predicts survival as given by the base estimator. A survival function, its upper and lower confidence intervals can be returned for each sample of the dataframe X. Parameters: X ( pd.DataFrame ) \u2013 data frame with samples to generate predictions return_ci ( Bool ) \u2013 whether to include confidence intervals ci_width ( Float ) \u2013 width of confidence interval Returns: ([(pd.DataFrame, np.array, np.array), pd.DataFrame]) \u2013 preds_df: A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci: upper confidence interval for the survival probability values lower_ci: lower confidence interval for the survival probability values","title":"Meta"},{"location":"meta.html#xgbse._meta.XGBSEBootstrapEstimator","text":"Bootstrap meta-estimator for XGBSE models: -- allows for confidence interval estimation in BCE -- provides variance stabilization for all models, mainly Kaplan Tree Performs simple bootstrap with sample size equal to training set size.","title":"xgbse._meta.XGBSEBootstrapEstimator"},{"location":"meta.html#xgbse._meta.XGBSEBootstrapEstimator.__init__","text":"Parameters: base_estimator ( XGBSEBaseEstimator ) \u2013 base estimator for bootstrap procedure n_estimators ( Int ) \u2013 number of estimators to fit in bootstrap procedure random_state ( Int ) \u2013 random state for resampling function","title":"__init__()"},{"location":"meta.html#xgbse._meta.XGBSEBootstrapEstimator.fit","text":"Fit several (base) estimators and store them. Parameters: X ( [pd.DataFrame, np.array] ) \u2013 features to be used while fitting XGBoost model y ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. **kwargs (``) \u2013 keyword arguments to be passed to .fit() method of base_estimator Returns: XGBSEBootstrapEstimator \u2013 Trained instance of XGBSEBootstrapEstimator","title":"fit()"},{"location":"meta.html#xgbse._meta.XGBSEBootstrapEstimator.predict","text":"Predicts survival as given by the base estimator. A survival function, its upper and lower confidence intervals can be returned for each sample of the dataframe X. Parameters: X ( pd.DataFrame ) \u2013 data frame with samples to generate predictions return_ci ( Bool ) \u2013 whether to include confidence intervals ci_width ( Float ) \u2013 width of confidence interval Returns: ([(pd.DataFrame, np.array, np.array), pd.DataFrame]) \u2013 preds_df: A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci: upper confidence interval for the survival probability values lower_ci: lower confidence interval for the survival probability values","title":"predict()"},{"location":"metrics.html","text":"\u00b6 approx_brier_score(y_true, survival, aggregate='mean') \u00b6 Estimate brier score for all survival time windows. Aggregate scores for an approximate integrated brier score estimate. Parameters: y_true ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. survival ( [pd.DataFrame, np.array] ) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. aggregate ( [string, None] ) \u2013 How to aggregate brier scores from different time windows 'mean' takes simple average None returns full list of brier scores for each time window Returns: [Float, np.array] \u2013 single value if aggregate is 'mean' np.array if aggregate is None concordance_index(y_true, survival, risk_strategy='mean', which_window=None) \u00b6 Compute the C-index for a structured array of ground truth times and events and a predicted survival curve using different strategies for estimating risk from it Parameters: y_true ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. survival ( [pd.DataFrame, np.array] ) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. risk_strategy ( string ) \u2013 {'mean','window','midpoint','precomputed'}, default='mean' Stratetegy to compute risks from the survival curve. For a given sample: 'mean' averages probabilities across all times 'window': lets user choose on of the time windows available (by which_window argument) and uses probabilities of this specific window 'midpoint': selects the most central window of index int(survival.columns.shape[0]/2) and uses probabilities of this specific window 'precomputed': assumes user has already calculated risk. The survival argument is assumed to contain an array of risks instead which_window ( object ) \u2013 which window to use when risk_strategy is 'window'. Should be one of the columns of the dataframe. Will raise ValueError if column is not present Returns: Float \u2013 Concordance index for y_true and survival dist_calibration_score(y_true, survival, n_bins=10, returns='pval') \u00b6 Estimate D-Calibration for the survival predictions. Parameters: y_true ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. survival ( [pd.DataFrame, np.array] ) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. n_bins ( Int ) \u2013 number of bins to equally divide the [0, 1] interval returns ( string ) \u2013 What information to return from the function 'statistic' returns the chi squared test statistic 'pval' returns the chi squared test p value 'max_deviation' returns the maximum percentage deviation from the expected value, calculated as abs(expected_percentage - real_percentage) 'histogram' returns the full calibration histogram per bin 'all' returns all of the above in a dictionary Returns: [Float, np.array, Dict] \u2013 single value if returns is in ['statistic','pval','max_deviation'] np.array if returns is 'histogram' dict if returns is 'all'","title":"Metrics"},{"location":"metrics.html#xgbse.metrics","text":"","title":"xgbse.metrics"},{"location":"metrics.html#xgbse.metrics.approx_brier_score","text":"Estimate brier score for all survival time windows. Aggregate scores for an approximate integrated brier score estimate. Parameters: y_true ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. survival ( [pd.DataFrame, np.array] ) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. aggregate ( [string, None] ) \u2013 How to aggregate brier scores from different time windows 'mean' takes simple average None returns full list of brier scores for each time window Returns: [Float, np.array] \u2013 single value if aggregate is 'mean' np.array if aggregate is None","title":"approx_brier_score()"},{"location":"metrics.html#xgbse.metrics.concordance_index","text":"Compute the C-index for a structured array of ground truth times and events and a predicted survival curve using different strategies for estimating risk from it Parameters: y_true ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. survival ( [pd.DataFrame, np.array] ) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. risk_strategy ( string ) \u2013 {'mean','window','midpoint','precomputed'}, default='mean' Stratetegy to compute risks from the survival curve. For a given sample: 'mean' averages probabilities across all times 'window': lets user choose on of the time windows available (by which_window argument) and uses probabilities of this specific window 'midpoint': selects the most central window of index int(survival.columns.shape[0]/2) and uses probabilities of this specific window 'precomputed': assumes user has already calculated risk. The survival argument is assumed to contain an array of risks instead which_window ( object ) \u2013 which window to use when risk_strategy is 'window'. Should be one of the columns of the dataframe. Will raise ValueError if column is not present Returns: Float \u2013 Concordance index for y_true and survival","title":"concordance_index()"},{"location":"metrics.html#xgbse.metrics.dist_calibration_score","text":"Estimate D-Calibration for the survival predictions. Parameters: y_true ( structured array(numpy.bool_, numpy.number ) \u2013 binary event indicator as first field, and time of event or time of censoring as second field. survival ( [pd.DataFrame, np.array] ) \u2013 A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. n_bins ( Int ) \u2013 number of bins to equally divide the [0, 1] interval returns ( string ) \u2013 What information to return from the function 'statistic' returns the chi squared test statistic 'pval' returns the chi squared test p value 'max_deviation' returns the maximum percentage deviation from the expected value, calculated as abs(expected_percentage - real_percentage) 'histogram' returns the full calibration histogram per bin 'all' returns all of the above in a dictionary Returns: [Float, np.array, Dict] \u2013 single value if returns is in ['statistic','pval','max_deviation'] np.array if returns is 'histogram' dict if returns is 'all'","title":"dist_calibration_score()"},{"location":"benchmarks/benchmarks.html","text":"Benchmarks \u00b6 In the examples folder you'll find benchmarks comparing xgbse to other survival analysis methods. We show 6 metrics (see [9] for details): c-index : concordance index. Equivalent to AUC with censored data. dcal_max_dev : maximum decile deviation from calibrated distribution. dcal_pval : p-value from chi-square test checking for D-Calibration. If larger than 0.05 then the model is D-Calibrated. ibs : approximate integrated brier score, the average brier score across all time windows. inference_time : time to perform inference. training_time : time to perform training. We executed all methods with default parameters, except for num_boosting_rounds , which was set to 10 for vanilla XGBoost, and 1000 for xgbse , as vanilla XGBoost was overfitting with the same setting as xgbse . No early stopping was used for XGBoost. We show results below for five datasets. FLCHAIN \u00b6 model c-index dcal_max_dev dcal_pval ibs inference_time training_time Weibull AFT 0.789 0.013 0.849 0.099 0.01 0.84 Cox-PH 0.788 0.011 0.971 0.099 0.007 1.192 XGBSE - Debiased BCE 0.784 0.03 0.036 0.101 0.47 46.155 XGB - AFT 0.782 nan nan nan 0.001 0.065 XGBSE - Bootstrap Trees 0.781 0.009 0.985 0.1 0.425 17.498 XGB - Cox 0.779 nan nan nan 0.001 0.085 XGBSE - Kaplan Neighbors 0.769 0.02 0.732 0.103 5.807 31.366 XGBSE - Kaplan Tree 0.768 0.011 0.929 0.103 0.003 0.212 Conditional Survival Forest 0.761 0.03 0.031 0.106 109.553 1.103 METABRIC \u00b6 model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Debiased BCE 0.632 0.032 0.381 0.157 0.369 14.198 XGBSE - Kaplan Neighbors 0.627 0.024 0.525 0.156 0.791 25.679 XGBSE - Bootstrap Trees 0.624 0.024 0.563 0.155 0.466 11.956 Conditional Survival Forest 0.623 0.032 0.289 0.152 31.874 0.201 Weibull AFT 0.622 0.024 0.667 0.154 0.008 0.39 Cox-PH 0.622 0.026 0.567 0.154 0.004 0.217 XGB - Cox 0.619 nan nan nan 0.001 0.023 XGB - AFT 0.61 nan nan nan 0.001 0.024 XGBSE - Kaplan Tree 0.59 0.036 0.18 0.165 0.014 0.114 RRNLNPH \u00b6 model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Debiased BCE 0.827 0.044 0 0.098 0.982 128.98 XGBSE - Bootstrap Trees 0.826 0.035 0 0.097 0.419 40.945 XGB - Cox 0.826 nan nan nan 0.001 0.056 XGB - AFT 0.825 nan nan nan 0.001 0.051 XGBSE - Kaplan Neighbors 0.823 0.034 0 0.1 66.891 112.969 XGBSE - Kaplan Tree 0.821 0.044 0 0.101 0.005 0.506 Conditional Survival Forest 0.811 0.067 0 0.113 5786.26 17.024 Weibull AFT 0.787 0.057 0 0.136 0.008 0.262 Cox-PH 0.787 0.055 0 0.135 0.011 2.362 SAC3 \u00b6 model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Debiased BCE 0.699 0.038 0 0.162 4.067 689.045 Cox-PH 0.682 0.035 0 0.165 0.037 2.138 Weibull AFT 0.682 0.039 0 0.165 0.037 2.057 XGBSE - Bootstrap Trees 0.677 0.043 0 0.168 3.134 220.772 XGB - AFT 0.671 nan nan nan 0.003 0.735 XGB - Cox 0.67 nan nan nan 0.002 0.913 XGBSE - Kaplan Neighbors 0.631 0.038 0 0.186 1318.84 531.032 XGBSE - Kaplan Tree 0.631 0.034 0 0.191 0.069 2.717 Conditional Survival Forest 0.622 0.044 0 0.187 691.939 837.069 SUPPORT \u00b6 model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGB - Cox 0.612 nan nan nan 0.001 0.137 XGB - AFT 0.612 nan nan nan 0.001 0.05 XGBSE - Bootstrap Trees 0.607 0.103 0 0.188 0.524 19.814 XGBSE - Debiased BCE 0.607 0.119 0 0.19 1.221 62.017 XGBSE - Kaplan Tree 0.598 0.097 0 0.203 0.005 0.194 Conditional Survival Forest 0.595 0.166 0 0.195 115.486 2.626 Cox-PH 0.578 0.16 0 0.201 0.01 0.519 XGBSE - Kaplan Neighbors 0.578 0.11 0 0.202 8.933 49.236 Weibull AFT 0.576 0.138 0 0.201 0.009 0.624 General comments: XGBSEDebiasedBCE show the most promising results, being the best method in 3 datasets and competitive in other 2. Other xgbse methods show good results too. In particular XGBSEKaplanTree with XGBSEBootstrapEstimator shows promising results, pointing to a direction for further research. Linear methods such as the Weibull AFT and Cox-PH from lifelines are surprisingly strong, specially for datasets with a small number of samples. xgbse methods show competitive results to vanilla xgboost as measured by C-index, while showing good results for \"survival curve metrics\". Thus, we can use xgbse as a calibrated replacement to vanilla xgboost . Conditional Survival Forest does not show consistent results. Also, it shows very high inference time, higher than training, which is unusual. It is the less efficient method of the group, followed by XGBSEKaplanNeighbors . xgbse takes longer to fit than vanilla xgboost . Specially for XGBSEDebiasedBCE , we have to build N logistic regressions where N is the number of time windows we'll predict. In all cases we used N = 30.","title":"Benchmarks"},{"location":"benchmarks/benchmarks.html#benchmarks","text":"In the examples folder you'll find benchmarks comparing xgbse to other survival analysis methods. We show 6 metrics (see [9] for details): c-index : concordance index. Equivalent to AUC with censored data. dcal_max_dev : maximum decile deviation from calibrated distribution. dcal_pval : p-value from chi-square test checking for D-Calibration. If larger than 0.05 then the model is D-Calibrated. ibs : approximate integrated brier score, the average brier score across all time windows. inference_time : time to perform inference. training_time : time to perform training. We executed all methods with default parameters, except for num_boosting_rounds , which was set to 10 for vanilla XGBoost, and 1000 for xgbse , as vanilla XGBoost was overfitting with the same setting as xgbse . No early stopping was used for XGBoost. We show results below for five datasets.","title":"Benchmarks"},{"location":"benchmarks/benchmarks.html#flchain","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time Weibull AFT 0.789 0.013 0.849 0.099 0.01 0.84 Cox-PH 0.788 0.011 0.971 0.099 0.007 1.192 XGBSE - Debiased BCE 0.784 0.03 0.036 0.101 0.47 46.155 XGB - AFT 0.782 nan nan nan 0.001 0.065 XGBSE - Bootstrap Trees 0.781 0.009 0.985 0.1 0.425 17.498 XGB - Cox 0.779 nan nan nan 0.001 0.085 XGBSE - Kaplan Neighbors 0.769 0.02 0.732 0.103 5.807 31.366 XGBSE - Kaplan Tree 0.768 0.011 0.929 0.103 0.003 0.212 Conditional Survival Forest 0.761 0.03 0.031 0.106 109.553 1.103","title":"FLCHAIN"},{"location":"benchmarks/benchmarks.html#metabric","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Debiased BCE 0.632 0.032 0.381 0.157 0.369 14.198 XGBSE - Kaplan Neighbors 0.627 0.024 0.525 0.156 0.791 25.679 XGBSE - Bootstrap Trees 0.624 0.024 0.563 0.155 0.466 11.956 Conditional Survival Forest 0.623 0.032 0.289 0.152 31.874 0.201 Weibull AFT 0.622 0.024 0.667 0.154 0.008 0.39 Cox-PH 0.622 0.026 0.567 0.154 0.004 0.217 XGB - Cox 0.619 nan nan nan 0.001 0.023 XGB - AFT 0.61 nan nan nan 0.001 0.024 XGBSE - Kaplan Tree 0.59 0.036 0.18 0.165 0.014 0.114","title":"METABRIC"},{"location":"benchmarks/benchmarks.html#rrnlnph","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Debiased BCE 0.827 0.044 0 0.098 0.982 128.98 XGBSE - Bootstrap Trees 0.826 0.035 0 0.097 0.419 40.945 XGB - Cox 0.826 nan nan nan 0.001 0.056 XGB - AFT 0.825 nan nan nan 0.001 0.051 XGBSE - Kaplan Neighbors 0.823 0.034 0 0.1 66.891 112.969 XGBSE - Kaplan Tree 0.821 0.044 0 0.101 0.005 0.506 Conditional Survival Forest 0.811 0.067 0 0.113 5786.26 17.024 Weibull AFT 0.787 0.057 0 0.136 0.008 0.262 Cox-PH 0.787 0.055 0 0.135 0.011 2.362","title":"RRNLNPH"},{"location":"benchmarks/benchmarks.html#sac3","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Debiased BCE 0.699 0.038 0 0.162 4.067 689.045 Cox-PH 0.682 0.035 0 0.165 0.037 2.138 Weibull AFT 0.682 0.039 0 0.165 0.037 2.057 XGBSE - Bootstrap Trees 0.677 0.043 0 0.168 3.134 220.772 XGB - AFT 0.671 nan nan nan 0.003 0.735 XGB - Cox 0.67 nan nan nan 0.002 0.913 XGBSE - Kaplan Neighbors 0.631 0.038 0 0.186 1318.84 531.032 XGBSE - Kaplan Tree 0.631 0.034 0 0.191 0.069 2.717 Conditional Survival Forest 0.622 0.044 0 0.187 691.939 837.069","title":"SAC3"},{"location":"benchmarks/benchmarks.html#support","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGB - Cox 0.612 nan nan nan 0.001 0.137 XGB - AFT 0.612 nan nan nan 0.001 0.05 XGBSE - Bootstrap Trees 0.607 0.103 0 0.188 0.524 19.814 XGBSE - Debiased BCE 0.607 0.119 0 0.19 1.221 62.017 XGBSE - Kaplan Tree 0.598 0.097 0 0.203 0.005 0.194 Conditional Survival Forest 0.595 0.166 0 0.195 115.486 2.626 Cox-PH 0.578 0.16 0 0.201 0.01 0.519 XGBSE - Kaplan Neighbors 0.578 0.11 0 0.202 8.933 49.236 Weibull AFT 0.576 0.138 0 0.201 0.009 0.624 General comments: XGBSEDebiasedBCE show the most promising results, being the best method in 3 datasets and competitive in other 2. Other xgbse methods show good results too. In particular XGBSEKaplanTree with XGBSEBootstrapEstimator shows promising results, pointing to a direction for further research. Linear methods such as the Weibull AFT and Cox-PH from lifelines are surprisingly strong, specially for datasets with a small number of samples. xgbse methods show competitive results to vanilla xgboost as measured by C-index, while showing good results for \"survival curve metrics\". Thus, we can use xgbse as a calibrated replacement to vanilla xgboost . Conditional Survival Forest does not show consistent results. Also, it shows very high inference time, higher than training, which is unusual. It is the less efficient method of the group, followed by XGBSEKaplanNeighbors . xgbse takes longer to fit than vanilla xgboost . Specially for XGBSEDebiasedBCE , we have to build N logistic regressions where N is the number of time windows we'll predict. In all cases we used N = 30.","title":"SUPPORT"},{"location":"examples/how_xgbse_works.html","text":"How xgbse works \u00b6 In this section, we try to make a quick introduction to xgbse . Refer to this this Notebook for the full code and/or if you want a more practical introduction. What xgbse tries to solve \u00b6 The XGBoost implementation provides two methods for survival analysis: Cox and Accelerated Failure Time (AFT). When it comes to ordering individuals by risk, both show competitive performance (as measured by C-index) while being lightning fast. However, we can observe shortcomings when it comes to other desirable statistical properties. Specifically, three properties are of concern: prediction of survival curves rather than point estimates estimation of confidence intervals calibrated (unbiased) expected survival times Let us take the AFT implementation as an example. The model assumes an underlying distribution for times and events, controlled by the aft_loss_distribution and aft_loss_distribution_scale hyperparameters. By tweaking the aft_loss_distribution_scale hyperparameter we can build models with very different average predicted survival times, while maintaing ordering, with good C-index results: # loop to show different scale results for scale in [ 1.5 , 1.0 , 0.5 ]: # chaning parameter PARAMS_XGB_AFT [ 'aft_loss_distribution_scale' ] = scale # training model bst = xgb . train ( PARAMS_XGB_AFT , dtrain , num_boost_round = 1000 , early_stopping_rounds = 10 , evals = [( dval , 'val' )], verbose_eval = 0 ) # predicting and evaluating preds = bst . predict ( dval ) cind = concordance_index_censored ( y_valid [ 'c1' ], y_valid [ 'c2' ], 1 - preds ) print ( f \"aft_loss_distribution_scale: { scale } \" ) print ( f \"C-index: { cind [ 0 ] : .3f } \" ) print ( f \"Average survival time: { preds . mean () : .0f } days\" ) print ( \"----\" ) aft_loss_distribution_scale: 1.5 C-index: 0.645 Average survival time: 203 days ---- aft_loss_distribution_scale: 1.0 C-index: 0.648 Average survival time: 165 days ---- aft_loss_distribution_scale: 0.5 C-index: 0.646 Average survival time: 125 days ---- If we plot the average predictions alongside a unbiased survival estimator such as the Kaplan Meier we can check that for each step of 0.5 in aft_loss_distribution_scale we move roughly one decile to the right in the curve. So what predictions should we trust? Such sensitivity to hyperparameters ( 0.003 C-index variation yet 78 days difference) raises red flags for applications that are dependent on robust and calibrated time-to-event estimates, mining trust and preventing shipping survival analysis models to production. Leveraging xgboost as a feature transformer \u00b6 Although in need of an extension for statistical rigor, xgboost is still a powerhouse. C-index results show that the model can capture a great deal of signal, being competitive with the state of the art. We just need to adapt how we use it. Besides being leveraged for prediction tasks, Gradient Boosted Trees (GBTs) can also be used as feature transformers of the input data. Trees in the ensemble perform splits on features that discriminate the target, encoding the most relevant information for the task at hand in their structure. In particular, the terminal nodes (leaves) at each tree in the ensemble define a sparse supervised feature transformation (embedding) of the input data. This kind of tree ensemble embedding has very convenient properties: sparsity and high-dimensionality: trees deal with nonlinearity and cast original features to a sparse, high-dimensional embedding, which helps linear models perform well when trained on it. This allows a Logistic Regression trained on the embedding (as one-hot encoded leaf indices) to have comparable performance to the actual ensemble, with the added benefit of probability calibration (see [1], [2], and [3]) supervision: trees also work as a noise filter, performing splits only through features that have predictive power. Thus, the embedding actually has a lower intrinsic dimension than the input data. This mitigates the curse of dimensionality and allows a K-Nearest Neighbor model trained on the embedding (using hamming distance) to have comparable performance to the actual ensemble, with the added flexibility to apply any function over the neighbor-sets to get predictions. This arbitrary function can be, for instance, an unbiased survival estimator such as the Kaplan-Meier estimator (see [4]) We take advantage of these properties in different ways as we will show in the next subsections. XGBSEDebiasedBCE : logistic regressions, time windows, embedding as input \u00b6 Our first approach, XGBSEDebiasedBCE , takes inspiration from the multi-task logistic regression method in [5], the BCE approach in [6], and the probability calibration ideas from [1], [2] and [3]. It consists of training a set of logistic regressions on top of the embedding produced by xgboost , each predicting survival at different user-defined discrete time windows. The classifiers remove individuals as they are censored, with targets that are indicators of surviving at each window. The naive approach tends to give biased survival curves, due to the removal of censored individuals. Thus, we made some adaptations such that logistic regressions estimate the di/ni term (point probabilities) in the Kaplan-Meier formula and then use the KM estimator to get nearly unbiased survival curves. This way, we can get full survival curves from xgboost , and confidence intervals with minor adaptations (such as performing some rounds of bootstrap). Training and scoring of logistic regression models is efficient, being performed in parallel through joblib , so the model can scale to hundreds of thousands or millions of samples. XGBSEKaplanNeighbors : Kaplan-Meier on nearest neighbors \u00b6 As explained in the previous section, even though the embedding produced by xgboost is sparse and high dimensional, its intrisic dimensionality actually should be lower than the input data. This enables us to \"convert\" xgboost into a nearest neighbor model, where we use hamming distance to define similar elements as the ones that co-occurred the most at the ensemble terminal nodes. Then, at each neighbor-set we can get survival estimates with robust methods such as the Kaplan-Meier estimator. We recommend using dart as the booster to prevent any tree to dominate variance in the ensemble and break the leaf co-ocurrence similarity logic. We built a high-performing implementation of the KM estimator to calculate several survival curves in a vectorized fashion, including upper and lower confidence intervals based on the Exponential Greenwood formula. However, this method can be very expensive at scales of hundreds of thousands of samples, due to the nearest neighbor search, both on training (construction of search index) and scoring (actual search). XGBSEKaplanTree : single tree, and Kaplan-Meier on its leaves \u00b6 As a simplification to XGBSEKaplanNeighbors , we also provide a single tree implementation. Instead of doing expensive nearest neighbor searches, we fit a single tree via xgboost and calculate KM curves at each of its leaves. It is by far the most efficient implementation, able to scale to millions of examples easily. At fit time, the tree is built and all KM curves are pre-calculated, so that at scoring time a simple query will suffice to get the model's estimates. However, as we're fitting a single tree, predictive power may be worse. That could be a sensible tradeoff, but we also provide XGBSEBootstrapEstimator , a bootstrap abstraction where we can fit a forest of XGBSEKaplanTree 's to improve accuracy and reduce variance. Does it solve the problem? \u00b6 Now we return to the first example and check how XGBEmbedKaplanNeighbors performs: # loop to show different scale results for scale in [ 1.5 , 1.0 , 0.5 ]: # chaning parameter PARAMS_XGB_AFT [ 'aft_loss_distribution_scale' ] = scale # training model xgbse_model = XGBSEKaplanNeighbors ( PARAMS_XGB_AFT , n_neighbors = 30 ) xgbse_model . fit ( X_train , y_train , validation_data = ( X_valid , y_valid ), early_stopping_rounds = 10 , time_bins = TIME_BINS ) # predicting and evaluating preds = xgbse_model . predict ( X_valid ) cind = concordance_index_censored ( y_valid [ 'c1' ], y_valid [ 'c2' ], ( 1 - preds ) . mean ( axis = 1 )) avg_probs = preds [[ 30 , 90 , 150 ]] . mean () . values . round ( 4 ) . tolist () print ( f \"aft_loss_distribution_scale: { scale } \" ) print ( f \"C-index: { cind [ 0 ] : .3f } \" ) print ( f \"Average probability of survival at [30, 90, 150] days: { avg_probs } \" ) print ( \"----\" ) aft_loss_distribution_scale : 1.5 C - index : 0.640 Average probability of survival at [ 30 , 90 , 150 ] days : [ 0.9109 , 0.6854 , 0.528 ] ---- aft_loss_distribution_scale : 1.0 C - index : 0.644 Average probability of survival at [ 30 , 90 , 150 ] days : [ 0.9111 , 0.6889 , 0.5333 ] ---- aft_loss_distribution_scale : 0.5 C - index : 0.650 Average probability of survival at [ 30 , 90 , 150 ] days : [ 0.913 , 0.6904 , 0.5289 ] ---- As measured by the average probability of survival in 30, 90 and 150 days the model is very stable, showing similar calibration results independently of aft_loss_distribution_scale choice, with comparable (or a bit worse) C-index results. Visually, the comparison of the average model predictions to a Kaplan Meier yields much better results: No more point estimates and high variation! Although is too harsh to claim that the problem is solved, we believe that the package can be a good, more statistically robust alternative to survival analysis.","title":"How XGBSE works"},{"location":"examples/how_xgbse_works.html#how-xgbse-works","text":"In this section, we try to make a quick introduction to xgbse . Refer to this this Notebook for the full code and/or if you want a more practical introduction.","title":"How xgbse works"},{"location":"examples/how_xgbse_works.html#what-xgbse-tries-to-solve","text":"The XGBoost implementation provides two methods for survival analysis: Cox and Accelerated Failure Time (AFT). When it comes to ordering individuals by risk, both show competitive performance (as measured by C-index) while being lightning fast. However, we can observe shortcomings when it comes to other desirable statistical properties. Specifically, three properties are of concern: prediction of survival curves rather than point estimates estimation of confidence intervals calibrated (unbiased) expected survival times Let us take the AFT implementation as an example. The model assumes an underlying distribution for times and events, controlled by the aft_loss_distribution and aft_loss_distribution_scale hyperparameters. By tweaking the aft_loss_distribution_scale hyperparameter we can build models with very different average predicted survival times, while maintaing ordering, with good C-index results: # loop to show different scale results for scale in [ 1.5 , 1.0 , 0.5 ]: # chaning parameter PARAMS_XGB_AFT [ 'aft_loss_distribution_scale' ] = scale # training model bst = xgb . train ( PARAMS_XGB_AFT , dtrain , num_boost_round = 1000 , early_stopping_rounds = 10 , evals = [( dval , 'val' )], verbose_eval = 0 ) # predicting and evaluating preds = bst . predict ( dval ) cind = concordance_index_censored ( y_valid [ 'c1' ], y_valid [ 'c2' ], 1 - preds ) print ( f \"aft_loss_distribution_scale: { scale } \" ) print ( f \"C-index: { cind [ 0 ] : .3f } \" ) print ( f \"Average survival time: { preds . mean () : .0f } days\" ) print ( \"----\" ) aft_loss_distribution_scale: 1.5 C-index: 0.645 Average survival time: 203 days ---- aft_loss_distribution_scale: 1.0 C-index: 0.648 Average survival time: 165 days ---- aft_loss_distribution_scale: 0.5 C-index: 0.646 Average survival time: 125 days ---- If we plot the average predictions alongside a unbiased survival estimator such as the Kaplan Meier we can check that for each step of 0.5 in aft_loss_distribution_scale we move roughly one decile to the right in the curve. So what predictions should we trust? Such sensitivity to hyperparameters ( 0.003 C-index variation yet 78 days difference) raises red flags for applications that are dependent on robust and calibrated time-to-event estimates, mining trust and preventing shipping survival analysis models to production.","title":"What xgbse tries to solve"},{"location":"examples/how_xgbse_works.html#leveraging-xgboost-as-a-feature-transformer","text":"Although in need of an extension for statistical rigor, xgboost is still a powerhouse. C-index results show that the model can capture a great deal of signal, being competitive with the state of the art. We just need to adapt how we use it. Besides being leveraged for prediction tasks, Gradient Boosted Trees (GBTs) can also be used as feature transformers of the input data. Trees in the ensemble perform splits on features that discriminate the target, encoding the most relevant information for the task at hand in their structure. In particular, the terminal nodes (leaves) at each tree in the ensemble define a sparse supervised feature transformation (embedding) of the input data. This kind of tree ensemble embedding has very convenient properties: sparsity and high-dimensionality: trees deal with nonlinearity and cast original features to a sparse, high-dimensional embedding, which helps linear models perform well when trained on it. This allows a Logistic Regression trained on the embedding (as one-hot encoded leaf indices) to have comparable performance to the actual ensemble, with the added benefit of probability calibration (see [1], [2], and [3]) supervision: trees also work as a noise filter, performing splits only through features that have predictive power. Thus, the embedding actually has a lower intrinsic dimension than the input data. This mitigates the curse of dimensionality and allows a K-Nearest Neighbor model trained on the embedding (using hamming distance) to have comparable performance to the actual ensemble, with the added flexibility to apply any function over the neighbor-sets to get predictions. This arbitrary function can be, for instance, an unbiased survival estimator such as the Kaplan-Meier estimator (see [4]) We take advantage of these properties in different ways as we will show in the next subsections.","title":"Leveraging xgboost as a feature transformer"},{"location":"examples/how_xgbse_works.html#xgbsedebiasedbce-logistic-regressions-time-windows-embedding-as-input","text":"Our first approach, XGBSEDebiasedBCE , takes inspiration from the multi-task logistic regression method in [5], the BCE approach in [6], and the probability calibration ideas from [1], [2] and [3]. It consists of training a set of logistic regressions on top of the embedding produced by xgboost , each predicting survival at different user-defined discrete time windows. The classifiers remove individuals as they are censored, with targets that are indicators of surviving at each window. The naive approach tends to give biased survival curves, due to the removal of censored individuals. Thus, we made some adaptations such that logistic regressions estimate the di/ni term (point probabilities) in the Kaplan-Meier formula and then use the KM estimator to get nearly unbiased survival curves. This way, we can get full survival curves from xgboost , and confidence intervals with minor adaptations (such as performing some rounds of bootstrap). Training and scoring of logistic regression models is efficient, being performed in parallel through joblib , so the model can scale to hundreds of thousands or millions of samples.","title":"XGBSEDebiasedBCE: logistic regressions, time windows, embedding as input"},{"location":"examples/how_xgbse_works.html#xgbsekaplanneighbors-kaplan-meier-on-nearest-neighbors","text":"As explained in the previous section, even though the embedding produced by xgboost is sparse and high dimensional, its intrisic dimensionality actually should be lower than the input data. This enables us to \"convert\" xgboost into a nearest neighbor model, where we use hamming distance to define similar elements as the ones that co-occurred the most at the ensemble terminal nodes. Then, at each neighbor-set we can get survival estimates with robust methods such as the Kaplan-Meier estimator. We recommend using dart as the booster to prevent any tree to dominate variance in the ensemble and break the leaf co-ocurrence similarity logic. We built a high-performing implementation of the KM estimator to calculate several survival curves in a vectorized fashion, including upper and lower confidence intervals based on the Exponential Greenwood formula. However, this method can be very expensive at scales of hundreds of thousands of samples, due to the nearest neighbor search, both on training (construction of search index) and scoring (actual search).","title":"XGBSEKaplanNeighbors: Kaplan-Meier on nearest neighbors"},{"location":"examples/how_xgbse_works.html#xgbsekaplantree-single-tree-and-kaplan-meier-on-its-leaves","text":"As a simplification to XGBSEKaplanNeighbors , we also provide a single tree implementation. Instead of doing expensive nearest neighbor searches, we fit a single tree via xgboost and calculate KM curves at each of its leaves. It is by far the most efficient implementation, able to scale to millions of examples easily. At fit time, the tree is built and all KM curves are pre-calculated, so that at scoring time a simple query will suffice to get the model's estimates. However, as we're fitting a single tree, predictive power may be worse. That could be a sensible tradeoff, but we also provide XGBSEBootstrapEstimator , a bootstrap abstraction where we can fit a forest of XGBSEKaplanTree 's to improve accuracy and reduce variance.","title":"XGBSEKaplanTree: single tree, and Kaplan-Meier on its leaves"},{"location":"examples/how_xgbse_works.html#does-it-solve-the-problem","text":"Now we return to the first example and check how XGBEmbedKaplanNeighbors performs: # loop to show different scale results for scale in [ 1.5 , 1.0 , 0.5 ]: # chaning parameter PARAMS_XGB_AFT [ 'aft_loss_distribution_scale' ] = scale # training model xgbse_model = XGBSEKaplanNeighbors ( PARAMS_XGB_AFT , n_neighbors = 30 ) xgbse_model . fit ( X_train , y_train , validation_data = ( X_valid , y_valid ), early_stopping_rounds = 10 , time_bins = TIME_BINS ) # predicting and evaluating preds = xgbse_model . predict ( X_valid ) cind = concordance_index_censored ( y_valid [ 'c1' ], y_valid [ 'c2' ], ( 1 - preds ) . mean ( axis = 1 )) avg_probs = preds [[ 30 , 90 , 150 ]] . mean () . values . round ( 4 ) . tolist () print ( f \"aft_loss_distribution_scale: { scale } \" ) print ( f \"C-index: { cind [ 0 ] : .3f } \" ) print ( f \"Average probability of survival at [30, 90, 150] days: { avg_probs } \" ) print ( \"----\" ) aft_loss_distribution_scale : 1.5 C - index : 0.640 Average probability of survival at [ 30 , 90 , 150 ] days : [ 0.9109 , 0.6854 , 0.528 ] ---- aft_loss_distribution_scale : 1.0 C - index : 0.644 Average probability of survival at [ 30 , 90 , 150 ] days : [ 0.9111 , 0.6889 , 0.5333 ] ---- aft_loss_distribution_scale : 0.5 C - index : 0.650 Average probability of survival at [ 30 , 90 , 150 ] days : [ 0.913 , 0.6904 , 0.5289 ] ---- As measured by the average probability of survival in 30, 90 and 150 days the model is very stable, showing similar calibration results independently of aft_loss_distribution_scale choice, with comparable (or a bit worse) C-index results. Visually, the comparison of the average model predictions to a Kaplan Meier yields much better results: No more point estimates and high variation! Although is too harsh to claim that the problem is solved, we believe that the package can be a good, more statistically robust alternative to survival analysis.","title":"Does it solve the problem?"}]}