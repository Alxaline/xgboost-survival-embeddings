<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Loft Data Science Team">
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>How XGBSE works - XGBoost Survival Embeddings</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "How XGBSE works";
    var mkdocs_page_input_path = "examples/how_xgbse_works.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> XGBoost Survival Embeddings</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../index.html">Home</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Modules</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../base.html">Base</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../bce.html">BCE</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../kaplan.html">Kaplan</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../meta.html">Meta</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../extrapolation.html">Extrapolation</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../converters.html">Converters</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../metrics.html">Metrics</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Examples</span></p>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="how_xgbse_works.html">How XGBSE works</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#what-xgbse-tries-to-solve">What xgbse tries to solve</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#leveraging-xgboost-as-a-feature-transformer">Leveraging xgboost as a feature transformer</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#xgbsedebiasedbce-logistic-regressions-time-windows-embedding-as-input">XGBSEDebiasedBCE: logistic regressions, time windows, embedding as input</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#xgbsekaplanneighbors-kaplan-meier-on-nearest-neighbors">XGBSEKaplanNeighbors: Kaplan-Meier on nearest neighbors</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#xgbsekaplantree-single-tree-and-kaplan-meier-on-its-leaves">XGBSEKaplanTree: single tree, and Kaplan-Meier on its leaves</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#does-it-solve-the-problem">Does it solve the problem?</a>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../benchmarks/benchmarks.html">Benchmarks</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">XGBoost Survival Embeddings</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Examples &raquo;</li>
        
      
    
    <li>How XGBSE works</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/loft-br/xgboost-survival-embeddings/edit/master/docs/examples/how_xgbse_works.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="how-xgbse-works">How <code>xgbse</code> works<a class="headerlink" href="#how-xgbse-works" title="Permanent link">&para;</a></h2>
<p>In this section, we try to make a quick introduction to <code>xgbse</code>. Refer to this <a href="https://github.com/loft-br/xgboost-survival-embeddings/blob/main/examples/how_xgbse_works.ipynb">this Notebook</a> for the full code and/or if you want a more practical introduction.</p>
<h3 id="what-xgbse-tries-to-solve"><em>What <code>xgbse</code> tries to solve</em><a class="headerlink" href="#what-xgbse-tries-to-solve" title="Permanent link">&para;</a></h3>
<p>The XGBoost implementation provides two methods for survival analysis: Cox and Accelerated Failure Time (AFT). When it comes to ordering individuals by risk, both show competitive performance (as measured by C-index) while being lightning fast.</p>
<p>However, we can observe shortcomings when it comes to other desirable statistical properties. Specifically, three properties are of concern:</p>
<ul>
<li>prediction of survival curves rather than point estimates</li>
<li>estimation of confidence intervals</li>
<li>calibrated (unbiased) expected survival times</li>
</ul>
<p>Let us take the AFT implementation as an example. The model assumes an underlying distribution for times and events, controlled by the <code>aft_loss_distribution</code> and <code>aft_loss_distribution_scale</code> hyperparameters. By tweaking the <code>aft_loss_distribution_scale</code> hyperparameter we can build models with very different average predicted survival times, while maintaing ordering, with good C-index results:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># loop to show different scale results</span>
<span class="k">for</span> <span class="n">scale</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]:</span>

    <span class="c1"># chaning parameter</span>
    <span class="n">PARAMS_XGB_AFT</span><span class="p">[</span><span class="s1">&#39;aft_loss_distribution_scale&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scale</span>

    <span class="c1"># training model</span>
    <span class="n">bst</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
        <span class="n">PARAMS_XGB_AFT</span><span class="p">,</span>
        <span class="n">dtrain</span><span class="p">,</span>
        <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">evals</span><span class="o">=</span><span class="p">[(</span><span class="n">dval</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">)],</span>
        <span class="n">verbose_eval</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>

    <span class="c1"># predicting and evaluating</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">bst</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dval</span><span class="p">)</span>
    <span class="n">cind</span> <span class="o">=</span> <span class="n">concordance_index_censored</span><span class="p">(</span><span class="n">y_valid</span><span class="p">[</span><span class="s1">&#39;c1&#39;</span><span class="p">],</span> <span class="n">y_valid</span><span class="p">[</span><span class="s1">&#39;c2&#39;</span><span class="p">],</span> <span class="mi">1</span><span class="o">-</span><span class="n">preds</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aft_loss_distribution_scale: </span><span class="si">{</span><span class="n">scale</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;C-index: </span><span class="si">{</span><span class="n">cind</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Average survival time: </span><span class="si">{</span><span class="n">preds</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> days&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;----&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>aft_loss_distribution_scale: 1.5
C-index: 0.645
Average survival time: 203 days
----
aft_loss_distribution_scale: 1.0
C-index: 0.648
Average survival time: 165 days
----
aft_loss_distribution_scale: 0.5
C-index: 0.646
Average survival time: 125 days
----
</code></pre></div>

<p>If we plot the average predictions alongside a unbiased survival estimator such as the Kaplan Meier we can check that for each step of <code>0.5</code> in <code>aft_loss_distribution_scale</code> we move roughly one decile to the right in the curve.</p>
<p><img src="img/avg_preds_vs_kaplan_xgb.png"></p>
<p>So what predictions should we trust? Such sensitivity to hyperparameters (<code>0.003</code> C-index variation yet 78 days difference) raises red flags for applications that are dependent on robust and calibrated time-to-event estimates, mining trust and preventing shipping survival analysis models to production.</p>
<h3 id="leveraging-xgboost-as-a-feature-transformer"><em>Leveraging <code>xgboost</code> as a feature transformer</em><a class="headerlink" href="#leveraging-xgboost-as-a-feature-transformer" title="Permanent link">&para;</a></h3>
<p>Although in need of an extension for statistical rigor, <code>xgboost</code> is still a powerhouse. C-index results show that the model can capture a great deal of signal, being competitive with the state of the art. We just need to adapt how we use it.</p>
<p>Besides being leveraged for prediction tasks, Gradient Boosted Trees (GBTs) can also be used as <strong><em>feature transformers</em></strong> of the input data. Trees in the ensemble perform splits on features that discriminate the target, encoding the most relevant information for the task at hand in their structure. In particular, the terminal nodes (leaves) at each tree in the ensemble define a <strong><em><a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html">sparse supervised feature transformation</a> (embedding)</em></strong> of the input data.</p>
<p>This kind of <em>tree ensemble embedding</em> has very convenient properties:</p>
<ol>
<li>
<p><strong>sparsity and high-dimensionality:</strong> trees deal with nonlinearity and cast original features to a sparse, high-dimensional embedding, which helps linear models perform well when trained on it. This allows a <strong>Logistic Regression</strong> trained on the embedding (as one-hot encoded leaf indices) to have comparable performance to the actual ensemble, with the added benefit of probability calibration (see [1], [2], and [3])</p>
</li>
<li>
<p><strong>supervision:</strong> trees also work as a noise filter, performing splits only through features that have predictive power. Thus, the embedding actually has a lower intrinsic dimension than the input data. This mitigates the curse of dimensionality and allows a <strong>K-Nearest Neighbor</strong> model trained on the embedding (using hamming distance) to have comparable performance to the actual ensemble, with the added flexibility to apply any function over the neighbor-sets to get predictions. This arbitrary function can be, for instance, an unbiased survival estimator such as the Kaplan-Meier estimator (see [4])</p>
</li>
</ol>
<p>We take advantage of these properties in different ways as we will show in the next subsections.</p>
<h3 id="xgbsedebiasedbce-logistic-regressions-time-windows-embedding-as-input"><em><code>XGBSEDebiasedBCE</code>: logistic regressions, time windows, embedding as input</em><a class="headerlink" href="#xgbsedebiasedbce-logistic-regressions-time-windows-embedding-as-input" title="Permanent link">&para;</a></h3>
<p>Our first approach, <code>XGBSEDebiasedBCE</code>, takes inspiration from the multi-task logistic regression method in [5], the BCE approach in [6], and the probability calibration ideas from [1], [2] and [3].</p>
<p>It consists of training a set of logistic regressions on top of the embedding produced by <code>xgboost</code>, each predicting survival at different user-defined discrete time windows. The classifiers remove individuals as they are censored, with targets that are indicators of surviving at each window.</p>
<p><img src="img/xgb_bce_diagram.svg"></p>
<p>The naive approach tends to give biased survival curves, due to the removal of censored individuals. Thus, we made some adaptations such that logistic regressions estimate the <code>di/ni</code> term (point probabilities) in the <a href="https://www.math.wustl.edu/~sawyer/handouts/greenwood.pdf">Kaplan-Meier formula</a> and then use the KM estimator to get nearly unbiased survival curves.</p>
<p>This way, we can get full survival curves from <code>xgboost</code>, and confidence intervals with minor adaptations (such as performing some rounds of bootstrap).</p>
<p>Training and scoring of logistic regression models is efficient, being performed in parallel through <code>joblib</code>, so the model can scale to hundreds of thousands or millions of samples.</p>
<h3 id="xgbsekaplanneighbors-kaplan-meier-on-nearest-neighbors"><em><code>XGBSEKaplanNeighbors</code>: Kaplan-Meier on nearest neighbors</em><a class="headerlink" href="#xgbsekaplanneighbors-kaplan-meier-on-nearest-neighbors" title="Permanent link">&para;</a></h3>
<p>As explained in the previous section, even though the embedding produced by <code>xgboost</code> is sparse and high dimensional, its intrisic dimensionality actually should be lower than the input data. This enables us to "convert" <code>xgboost</code> into a nearest neighbor model, where we use hamming distance to define similar elements as the ones that co-occurred the most at the ensemble terminal nodes. Then, at each neighbor-set we can get survival estimates with robust methods such as the Kaplan-Meier estimator.</p>
<p><img src="img/xgb_neighbors_diagram.svg"></p>
<p>We recommend using <code>dart</code> as the booster to prevent any tree to dominate variance in the ensemble and break the leaf co-ocurrence similarity logic. We built a high-performing implementation of the KM estimator to calculate several survival curves in a vectorized fashion, including upper and lower confidence intervals based on the Exponential Greenwood formula.</p>
<p>However, this method can be very expensive at scales of hundreds of thousands of samples, due to the nearest neighbor search, both on training (construction of search index) and scoring (actual search).</p>
<h3 id="xgbsekaplantree-single-tree-and-kaplan-meier-on-its-leaves"><em><code>XGBSEKaplanTree</code>: single tree, and Kaplan-Meier on its leaves</em><a class="headerlink" href="#xgbsekaplantree-single-tree-and-kaplan-meier-on-its-leaves" title="Permanent link">&para;</a></h3>
<p>As a simplification to <code>XGBSEKaplanNeighbors</code>, we also provide a single tree implementation. Instead of doing expensive nearest neighbor searches, we fit a single tree via <code>xgboost</code> and calculate KM curves at each of its leaves.</p>
<p><img src="img/xgb_tree_diagram.svg"></p>
<p>It is by far the most efficient implementation, able to scale to millions of examples easily. At fit time, the tree is built and all KM curves are pre-calculated, so that at scoring time a simple query will suffice to get the model's estimates.</p>
<p>However, as we're fitting a single tree, predictive power may be worse. That could be a sensible tradeoff, but we also provide <code>XGBSEBootstrapEstimator</code>, a bootstrap abstraction where we can fit a forest of <code>XGBSEKaplanTree</code>'s to improve accuracy and reduce variance.</p>
<h3 id="does-it-solve-the-problem"><em>Does it solve the problem?</em><a class="headerlink" href="#does-it-solve-the-problem" title="Permanent link">&para;</a></h3>
<p>Now we return to the first example and check how <code>XGBEmbedKaplanNeighbors</code> performs:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># loop to show different scale results</span>
<span class="k">for</span> <span class="n">scale</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]:</span>

    <span class="c1"># chaning parameter</span>
    <span class="n">PARAMS_XGB_AFT</span><span class="p">[</span><span class="s1">&#39;aft_loss_distribution_scale&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scale</span>

    <span class="c1"># training model</span>
    <span class="n">xgbse_model</span> <span class="o">=</span> <span class="n">XGBSEKaplanNeighbors</span><span class="p">(</span><span class="n">PARAMS_XGB_AFT</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">xgbse_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
        <span class="n">validation_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
        <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">time_bins</span><span class="o">=</span><span class="n">TIME_BINS</span>
    <span class="p">)</span>

    <span class="c1"># predicting and evaluating</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">xgbse_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
    <span class="n">cind</span> <span class="o">=</span> <span class="n">concordance_index_censored</span><span class="p">(</span><span class="n">y_valid</span><span class="p">[</span><span class="s1">&#39;c1&#39;</span><span class="p">],</span> <span class="n">y_valid</span><span class="p">[</span><span class="s1">&#39;c2&#39;</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">avg_probs</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">150</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aft_loss_distribution_scale: </span><span class="si">{</span><span class="n">scale</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;C-index: </span><span class="si">{</span><span class="n">cind</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Average probability of survival at [30, 90, 150] days: </span><span class="si">{</span><span class="n">avg_probs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;----&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">aft_loss_distribution_scale</span><span class="o">:</span> <span class="mf">1.5</span>
<span class="n">C</span><span class="o">-</span><span class="n">index</span><span class="o">:</span> <span class="mf">0.640</span>
<span class="n">Average</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">survival</span> <span class="n">at</span> <span class="o">[</span><span class="mi">30</span><span class="o">,</span> <span class="mi">90</span><span class="o">,</span> <span class="mi">150</span><span class="o">]</span> <span class="n">days</span><span class="o">:</span> <span class="o">[</span><span class="mf">0.9109</span><span class="o">,</span> <span class="mf">0.6854</span><span class="o">,</span> <span class="mf">0.528</span><span class="o">]</span>
<span class="o">----</span>
<span class="n">aft_loss_distribution_scale</span><span class="o">:</span> <span class="mf">1.0</span>
<span class="n">C</span><span class="o">-</span><span class="n">index</span><span class="o">:</span> <span class="mf">0.644</span>
<span class="n">Average</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">survival</span> <span class="n">at</span> <span class="o">[</span><span class="mi">30</span><span class="o">,</span> <span class="mi">90</span><span class="o">,</span> <span class="mi">150</span><span class="o">]</span> <span class="n">days</span><span class="o">:</span> <span class="o">[</span><span class="mf">0.9111</span><span class="o">,</span> <span class="mf">0.6889</span><span class="o">,</span> <span class="mf">0.5333</span><span class="o">]</span>
<span class="o">----</span>
<span class="n">aft_loss_distribution_scale</span><span class="o">:</span> <span class="mf">0.5</span>
<span class="n">C</span><span class="o">-</span><span class="n">index</span><span class="o">:</span> <span class="mf">0.650</span>
<span class="n">Average</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">survival</span> <span class="n">at</span> <span class="o">[</span><span class="mi">30</span><span class="o">,</span> <span class="mi">90</span><span class="o">,</span> <span class="mi">150</span><span class="o">]</span> <span class="n">days</span><span class="o">:</span> <span class="o">[</span><span class="mf">0.913</span><span class="o">,</span> <span class="mf">0.6904</span><span class="o">,</span> <span class="mf">0.5289</span><span class="o">]</span>
<span class="o">----</span>
</code></pre></div>

<p>As measured by the average probability of survival in 30, 90 and 150 days the model is very stable, showing similar calibration results independently of <code>aft_loss_distribution_scale</code> choice, with comparable (or a bit worse) C-index results. Visually, the comparison of the average model predictions to a Kaplan Meier yields much better results:</p>
<p><img src="img/avg_preds_vs_kaplan_xgbse.png"></p>
<p>No more point estimates and high variation! Although is too harsh to claim that the problem is solved, we believe that the package can be a good, more statistically robust alternative to survival analysis.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../benchmarks/benchmarks.html" class="btn btn-neutral float-right" title="Benchmarks">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../metrics.html" class="btn btn-neutral" title="Metrics"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/loft-br/xgboost-survival-embeddings/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../metrics.html" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../benchmarks/benchmarks.html" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
